{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF4bkoTzFtNJ",
        "outputId": "33c6d662-50a7-4729-8dc4-7319f6478db5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture: [64, 1], Activation Function: relu, Learning Rate: 0.001\n",
            "Validation Loss: 3.9237\n",
            "\n",
            "Architecture: [64, 1], Activation Function: relu, Learning Rate: 0.01\n",
            "Validation Loss: 1.0346\n",
            "\n",
            "Architecture: [64, 1], Activation Function: relu, Learning Rate: 0.1\n",
            "Validation Loss: 0.4602\n",
            "\n",
            "Architecture: [64, 1], Activation Function: tanh, Learning Rate: 0.001\n",
            "Validation Loss: 3.5216\n",
            "\n",
            "Architecture: [64, 1], Activation Function: tanh, Learning Rate: 0.01\n",
            "Validation Loss: 0.8417\n",
            "\n",
            "Architecture: [64, 1], Activation Function: tanh, Learning Rate: 0.1\n",
            "Validation Loss: 0.5510\n",
            "\n",
            "Architecture: [64, 1], Activation Function: sigmoid, Learning Rate: 0.001\n",
            "Validation Loss: 3.5505\n",
            "\n",
            "Architecture: [64, 1], Activation Function: sigmoid, Learning Rate: 0.01\n",
            "Validation Loss: 0.8226\n",
            "\n",
            "Architecture: [64, 1], Activation Function: sigmoid, Learning Rate: 0.1\n",
            "Validation Loss: 0.4826\n",
            "\n",
            "Architecture: [128, 1], Activation Function: relu, Learning Rate: 0.001\n",
            "Validation Loss: 3.7046\n",
            "\n",
            "Architecture: [128, 1], Activation Function: relu, Learning Rate: 0.01\n",
            "Validation Loss: 0.7901\n",
            "\n",
            "Architecture: [128, 1], Activation Function: relu, Learning Rate: 0.1\n",
            "Validation Loss: 0.5300\n",
            "\n",
            "Architecture: [128, 1], Activation Function: tanh, Learning Rate: 0.001\n",
            "Validation Loss: 3.7109\n",
            "\n",
            "Architecture: [128, 1], Activation Function: tanh, Learning Rate: 0.01\n",
            "Validation Loss: 0.5298\n",
            "\n",
            "Architecture: [128, 1], Activation Function: tanh, Learning Rate: 0.1\n",
            "Validation Loss: 0.7805\n",
            "\n",
            "Architecture: [128, 1], Activation Function: sigmoid, Learning Rate: 0.001\n",
            "Validation Loss: 2.3636\n",
            "\n",
            "Architecture: [128, 1], Activation Function: sigmoid, Learning Rate: 0.01\n",
            "Validation Loss: 1.1025\n",
            "\n",
            "Architecture: [128, 1], Activation Function: sigmoid, Learning Rate: 0.1\n",
            "Validation Loss: 0.9339\n",
            "\n",
            "Architecture: [64, 64, 1], Activation Function: relu, Learning Rate: 0.001\n",
            "Validation Loss: 3.3041\n",
            "\n",
            "Architecture: [64, 64, 1], Activation Function: relu, Learning Rate: 0.01\n",
            "Validation Loss: 0.6347\n",
            "\n",
            "Architecture: [64, 64, 1], Activation Function: relu, Learning Rate: 0.1\n",
            "Validation Loss: 3.0030\n",
            "\n",
            "Architecture: [64, 64, 1], Activation Function: tanh, Learning Rate: 0.001\n",
            "Validation Loss: 1.7423\n",
            "\n",
            "Architecture: [64, 64, 1], Activation Function: tanh, Learning Rate: 0.01\n",
            "Validation Loss: 0.7419\n",
            "\n",
            "Architecture: [64, 64, 1], Activation Function: tanh, Learning Rate: 0.1\n",
            "Validation Loss: 1.0007\n",
            "\n",
            "Architecture: [64, 64, 1], Activation Function: sigmoid, Learning Rate: 0.001\n",
            "Validation Loss: 1.5574\n",
            "\n",
            "Architecture: [64, 64, 1], Activation Function: sigmoid, Learning Rate: 0.01\n",
            "Validation Loss: 1.2543\n",
            "\n",
            "Architecture: [64, 64, 1], Activation Function: sigmoid, Learning Rate: 0.1\n",
            "Validation Loss: 1.3534\n",
            "\n",
            "Architecture: [128, 64, 1], Activation Function: relu, Learning Rate: 0.001\n",
            "Validation Loss: 1.8951\n",
            "\n",
            "Architecture: [128, 64, 1], Activation Function: relu, Learning Rate: 0.01\n",
            "Validation Loss: 0.6127\n",
            "\n",
            "Architecture: [128, 64, 1], Activation Function: relu, Learning Rate: 0.1\n",
            "Validation Loss: 4.1484\n",
            "\n",
            "Architecture: [128, 64, 1], Activation Function: tanh, Learning Rate: 0.001\n",
            "Validation Loss: 0.8491\n",
            "\n",
            "Architecture: [128, 64, 1], Activation Function: tanh, Learning Rate: 0.01\n",
            "Validation Loss: 0.6836\n",
            "\n",
            "Architecture: [128, 64, 1], Activation Function: tanh, Learning Rate: 0.1\n",
            "Validation Loss: 0.9818\n",
            "\n",
            "Architecture: [128, 64, 1], Activation Function: sigmoid, Learning Rate: 0.001\n",
            "Validation Loss: 1.3241\n",
            "\n",
            "Architecture: [128, 64, 1], Activation Function: sigmoid, Learning Rate: 0.01\n",
            "Validation Loss: 1.3182\n",
            "\n",
            "Architecture: [128, 64, 1], Activation Function: sigmoid, Learning Rate: 0.1\n",
            "Validation Loss: 1.3155\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# prompt: build a similar code but use pytorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the California Housing dataset\n",
        "boston = fetch_california_housing()\n",
        "\n",
        "# Preprocess the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(boston.data)\n",
        "y = boston.target.reshape(-1, 1)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the neural network architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, layers, activation):\n",
        "        super(Net, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(len(layers) - 1):\n",
        "            self.layers.append(nn.Linear(layers[i], layers[i+1]))\n",
        "            if i < len(layers) - 2:\n",
        "              if activation == 'relu':\n",
        "                self.layers.append(nn.ReLU())\n",
        "              elif activation == 'tanh':\n",
        "                self.layers.append(nn.Tanh())\n",
        "              elif activation == 'sigmoid':\n",
        "                self.layers.append(nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "# Define a list of architectures to compare\n",
        "architectures = [\n",
        "    [X.shape[1], 64, 1],\n",
        "    [X.shape[1], 128, 1],\n",
        "    [X.shape[1], 64, 64, 1],\n",
        "    [X.shape[1], 128, 64, 1],\n",
        "]\n",
        "\n",
        "# Define a list of activation functions to compare\n",
        "activation_functions = [\n",
        "    'relu',\n",
        "    'tanh',\n",
        "    'sigmoid',\n",
        "]\n",
        "\n",
        "# Define a list of learning rates to compare\n",
        "learning_rates = [\n",
        "    0.001,\n",
        "    0.01,\n",
        "    0.1,\n",
        "]\n",
        "\n",
        "# Train and evaluate each model\n",
        "for architecture in architectures:\n",
        "    for activation_function in activation_functions:\n",
        "        for learning_rate in learning_rates:\n",
        "            # Define and train the model\n",
        "            model = Net(architecture, activation_function)\n",
        "            criterion = nn.MSELoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "            # Training loop\n",
        "            epochs = 20\n",
        "            for epoch in range(epochs):\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(X_train)\n",
        "                loss = criterion(outputs, y_train)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Evaluate the model\n",
        "            with torch.no_grad():\n",
        "              outputs_test = model(X_test)\n",
        "              val_loss = criterion(outputs_test, y_test)\n",
        "              print(f'Architecture: {architecture[1:]}, Activation Function: {activation_function}, Learning Rate: {learning_rate}')\n",
        "              print(f'Validation Loss: {val_loss:.4f}\\n')\n"
      ]
    }
  ]
}