{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZNHbMyvcaxe",
        "outputId": "1972a00a-201c-40d7-b5f5-181137fa64e1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (2024.9.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# %% [markdown]\n",
        "# # Molecule Generation Using GAN in PyTorch\n",
        "#\n",
        "# This notebook demonstrates an end-to-end workflow for generating novel molecules using a Generative Adversarial Network (GAN) implemented in PyTorch.\n",
        "#\n",
        "# **Workflow Overview:**\n",
        "#\n",
        "# 1. **Data Download & Preprocessing**\n",
        "#    - Download an online SMILES dataset.\n",
        "#    - Load and filter valid SMILES using RDKit.\n",
        "#    - Create a vocabulary for tokenizing SMILES.\n",
        "#    - Visualize the length distribution of SMILES strings.\n",
        "#\n",
        "# 2. **Dataset Creation**\n",
        "#    - Define a custom PyTorch Dataset that tokenizes and pads the SMILES.\n",
        "#    - Load the dataset using DataLoader.\n",
        "#\n",
        "# 3. **GAN Model Architecture**\n",
        "#    - Define the Generator: maps latent vectors to SMILES sequences using an LSTM and Gumbel-Softmax.\n",
        "#    - Define the Discriminator: classifies one-hot encoded SMILES using an embedding layer and bidirectional LSTM.\n",
        "#\n",
        "# 4. **Training the GAN**\n",
        "#    - Train using an adversarial training loop.\n",
        "#    - Print and plot losses over epochs.\n",
        "#\n",
        "# 5. **Molecule Generation and Evaluation**\n",
        "#    - Generate new molecules using the trained Generator.\n",
        "#    - Convert the outputs back to SMILES strings.\n",
        "#    - Validate generated molecules using RDKit.\n",
        "#    - Plot a grid of valid molecules.\n",
        "#\n",
        "# Let's get started!"
      ],
      "metadata": {
        "id": "ahglddauhTF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import requests                     # For downloading datasets\n",
        "import numpy as np                  # For numerical operations\n",
        "import torch                        # Main PyTorch package\n",
        "import torch.nn as nn               # For neural network modules\n",
        "import torch.nn.functional as F     # For activation functions and utilities\n",
        "import matplotlib.pyplot as plt     # For plotting graphs and images\n",
        "from tqdm import tqdm               # For progress bars\n",
        "\n",
        "# RDKit for molecule processing and visualization\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "\n",
        "# Determine device for training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_eg-t4Z1unE",
        "outputId": "f1852dc8-0ebf-44e3-ebd2-a705d58c0979"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# ## 1. Data Download & Preprocessing\n",
        "#\n",
        "# In this section, we:\n",
        "#\n",
        "# - Download a sample SMILES dataset from an online repository.\n",
        "# - Load SMILES strings from the file.\n",
        "# - Filter out invalid SMILES using RDKit.\n",
        "# - Build a character-level vocabulary for tokenizing the SMILES.\n",
        "# - Plot the distribution of SMILES string lengths.\n",
        "#\n",
        "# **Note:** The dataset used here is a sample file from MolGANâ€™s repository.\n"
      ],
      "metadata": {
        "id": "d4zld_fx1ulF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "def download_dataset(url: str, save_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Downloads the dataset from the specified URL if not already present locally.\n",
        "    \"\"\"\n",
        "    if os.path.exists(save_path):\n",
        "        print(f\"Dataset already exists at {save_path}. Skipping download.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Downloading dataset from {url} ...\")\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_length = response.headers.get('content-length')\n",
        "\n",
        "    if total_length is None:\n",
        "        with open(save_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "    else:\n",
        "        total_length = int(total_length)\n",
        "        with open(save_path, 'wb') as f:\n",
        "            for data in tqdm(response.iter_content(chunk_size=4096), total=total_length//4096, desc=\"Downloading\"):\n",
        "                f.write(data)\n",
        "    print(f\"Dataset downloaded and saved to {save_path}.\")\n",
        "\n",
        "def load_smiles_dataset(file_path: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Loads SMILES strings from a text file (one SMILES per line).\n",
        "    \"\"\"\n",
        "    smiles_list = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                smiles_list.append(line)\n",
        "    print(f\"Loaded {len(smiles_list)} SMILES strings from {file_path}.\")\n",
        "    return smiles_list\n",
        "\n",
        "def filter_valid_smiles(smiles_list: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Filters out invalid SMILES strings using RDKit.\n",
        "    \"\"\"\n",
        "    valid_smiles = []\n",
        "    invalid_count = 0\n",
        "    for smi in smiles_list:\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is not None:\n",
        "            valid_smiles.append(smi)\n",
        "        else:\n",
        "            invalid_count += 1\n",
        "    print(f\"Filtered out {invalid_count} invalid SMILES. {len(valid_smiles)} valid SMILES remain.\")\n",
        "    return valid_smiles\n",
        "\n",
        "def create_vocabulary(smiles_list: List[str]) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Creates a character-to-index mapping from the SMILES strings.\n",
        "    \"\"\"\n",
        "    vocab = {}\n",
        "    for smi in smiles_list:\n",
        "        for char in smi:\n",
        "            if char not in vocab:\n",
        "                vocab[char] = len(vocab)\n",
        "    # Add special padding token.\n",
        "    if \"<PAD>\" not in vocab:\n",
        "        vocab[\"<PAD>\"] = len(vocab)\n",
        "    print(f\"Created vocabulary with {len(vocab)} tokens.\")\n",
        "    return vocab\n",
        "\n",
        "def tokenize_smiles(smiles: str, vocab: Dict[str, int]) -> List[int]:\n",
        "    \"\"\"\n",
        "    Tokenizes a SMILES string using the provided vocabulary.\n",
        "    \"\"\"\n",
        "    return [vocab[char] for char in smiles]\n",
        "\n",
        "def pad_sequence(seq: List[int], max_length: int, pad_value: int) -> List[int]:\n",
        "    \"\"\"\n",
        "    Pads or truncates a sequence to the maximum length.\n",
        "    \"\"\"\n",
        "    padded_seq = seq[:max_length]\n",
        "    padded_seq += [pad_value] * (max_length - len(padded_seq))\n",
        "    return padded_seq\n"
      ],
      "metadata": {
        "id": "Ll0rzvCV1ui8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Define the dataset URL and local filename.\n",
        "#dataset_url = \"https://raw.githubusercontent.com/tsudalab/MolGAN/master/data/zinc_sample.txt\"\n",
        "dataset_file = \"zinc_sample.txt\"\n",
        "\n",
        "# Download the dataset if needed.\n",
        "#download_dataset(dataset_url, dataset_file)\n",
        "\n",
        "# Load SMILES strings from file.\n",
        "smiles_list = load_smiles_dataset(dataset_file)\n",
        "\n",
        "# Filter out invalid SMILES.\n",
        "smiles_list = filter_valid_smiles(smiles_list)\n",
        "\n",
        "# Build the vocabulary for tokenizing SMILES.\n",
        "vocab = create_vocabulary(smiles_list)\n",
        "print(\"Vocabulary:\", vocab)\n",
        "\n",
        "# Create the inverse mapping (for decoding tokens later).\n",
        "idx_to_char = {idx: char for char, idx in vocab.items()}\n",
        "\n",
        "# Determine the maximum sequence length in the dataset.\n",
        "max_length = max(len(smi) for smi in smiles_list)\n",
        "print(\"Maximum SMILES length:\", max_length)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0vf1JP01ugl",
        "outputId": "9cfc1d17-0b9f-4dad-f49e-18cf42d7ff9e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 482 SMILES strings from zinc_sample.txt.\n",
            "Filtered out 0 invalid SMILES. 482 valid SMILES remain.\n",
            "Created vocabulary with 10 tokens.\n",
            "Vocabulary: {'C': 0, '1': 1, '=': 2, '(': 3, 'O': 4, ')': 5, '2': 6, '3': 7, '#': 8, '<PAD>': 9}\n",
            "Maximum SMILES length: 62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Visualize the distribution of SMILES string lengths.\n",
        "smiles_lengths = [len(smi) for smi in smiles_list]\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(smiles_lengths, bins=range(0, max(smiles_lengths) + 2, 1), color='skyblue', edgecolor='black')\n",
        "plt.title(\"Distribution of SMILES String Lengths\")\n",
        "plt.xlabel(\"Length\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 2. Create a Custom PyTorch Dataset\n",
        "#\n",
        "# We now create a custom PyTorch Dataset that:\n",
        "# - Tokenizes the SMILES strings.\n",
        "# - Pads them to a fixed maximum length.\n",
        "# - Converts them to tensors for model training.\n",
        "#\n",
        "# We then load the dataset using a DataLoader for batch processing.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "usC0mFY01ueE",
        "outputId": "188520cf-1cf0-4b55-f4fd-c053363db80f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MOUTSOsI4Ev9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SmilesDataset(Dataset):\n",
        "    def __init__(self, smiles_list: List[str], vocab: Dict[str, int], max_length: int):\n",
        "        self.vocab = vocab\n",
        "        self.max_length = max_length\n",
        "        self.pad_value = vocab[\"<PAD>\"]\n",
        "        self.smiles_list = smiles_list\n",
        "        # Tokenize and pad all SMILES strings.\n",
        "        self.tokenized_data = [pad_sequence(tokenize_smiles(smi, vocab), max_length, self.pad_value)\n",
        "                               for smi in smiles_list]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.tokenized_data[idx]\n",
        "        return torch.tensor(sample, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "IhoAZyRz1ubt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Create the dataset and DataLoader.\n",
        "batch_size = 64\n",
        "dataset = SmilesDataset(smiles_list, vocab, max_length)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "print(\"DataLoader created with batch size:\", batch_size)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. GAN Architecture: Generator and Discriminator\n",
        "#\n",
        "# We now define our GAN models:\n",
        "#\n",
        "# - **Generator:**\n",
        "#   Takes a latent noise vector and maps it through a fully connected layer and an LSTM to produce a sequence of logits over the vocabulary. We then apply the Gumbel-Softmax trick to obtain a one-hot style output that represents a SMILES string.\n",
        "#\n",
        "# - **Discriminator:**\n",
        "#   Converts one-hot encoded SMILES sequences into dense embeddings and uses a bidirectional LSTM to extract sequence features. The final linear layer outputs a probability that the input sequence is real.\n",
        "#\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZXA9S4X1uYt",
        "outputId": "dda73c1b-acec-47cf-dde8-33a7ff2929ba"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoader created with batch size: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim: int, hidden_dim: int, seq_length: int, vocab_size: int, temperature: float = 0.8):\n",
        "        super(Generator, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.seq_length = seq_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.temperature = temperature\n",
        "\n",
        "        # Project the latent vector to a sequence of hidden states.\n",
        "        self.fc = nn.Linear(latent_dim, hidden_dim * seq_length)\n",
        "        # LSTM for sequential modeling.\n",
        "        self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, num_layers=2, batch_first=True)\n",
        "        # Final projection to vocabulary logits.\n",
        "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size = z.size(0)\n",
        "        x = self.fc(z)\n",
        "        x = x.view(batch_size, self.seq_length, self.hidden_dim)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        logits = self.fc_out(lstm_out)\n",
        "        # Apply Gumbel-Softmax to sample differentiably from the logits.\n",
        "        gumbel_output = F.gumbel_softmax(logits, tau=self.temperature, hard=True, dim=-1)\n",
        "        return gumbel_output\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, seq_length: int):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        # Linear layer to convert one-hot vectors to embeddings.\n",
        "        self.embedding = nn.Linear(vocab_size, embed_dim)\n",
        "        # Bidirectional LSTM for extracting sequential features.\n",
        "        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, num_layers=2,\n",
        "                            batch_first=True, bidirectional=True)\n",
        "        # Final fully connected layer for classification.\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x_embed = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(x_embed)\n",
        "        lstm_last = torch.mean(lstm_out, dim=1)  # Use average pooling over time steps.\n",
        "        logits = self.fc(lstm_last)\n",
        "        prob = torch.sigmoid(logits)\n",
        "        return prob\n"
      ],
      "metadata": {
        "id": "_Dtrl7Xg18Vt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Set hyperparameters for our models.\n",
        "latent_dim = 100\n",
        "gen_hidden_dim = 256\n",
        "disc_hidden_dim = 256\n",
        "embed_dim = 128\n",
        "temperature = 0.8\n",
        "vocab_size = len(vocab)\n",
        "seq_length = max_length\n",
        "\n",
        "# Initialize the Generator and Discriminator.\n",
        "generator = Generator(latent_dim, gen_hidden_dim, seq_length, vocab_size, temperature).to(device)\n",
        "# For ease of use in training, add vocab_size as an attribute.\n",
        "generator.vocab_size = vocab_size\n",
        "discriminator = Discriminator(vocab_size, embed_dim, disc_hidden_dim, seq_length).to(device)\n",
        "\n",
        "print(\"Generator and Discriminator have been initialized.\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 4. Training the GAN\n",
        "#\n",
        "# In this section, we train the GAN using an adversarial training loop.\n",
        "#\n",
        "# **Training Details:**\n",
        "#\n",
        "# - **Discriminator Training:**\n",
        "#   The discriminator is trained on both real SMILES (converted to one-hot encoding) and fake SMILES generated by the Generator. Binary cross entropy loss is used.\n",
        "#\n",
        "# - **Generator Training:**\n",
        "#   The generator is trained to fool the discriminator, so we train it with the goal of getting the discriminator to predict \"real\" for its generated samples.\n",
        "#\n",
        "# The losses for both models are printed and stored for later visualization.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icGqywlu18S0",
        "outputId": "973e51e9-e1c2-4f70-8f24-3a4858cee755"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator and Discriminator have been initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "def train_gan(generator: nn.Module,\n",
        "              discriminator: nn.Module,\n",
        "              data_loader: DataLoader,\n",
        "              latent_dim: int,\n",
        "              num_epochs: int,\n",
        "              device: torch.device,\n",
        "              lr: float = 0.0002) -> Dict[str, List[float]]:\n",
        "    \"\"\"\n",
        "    Trains the GAN using adversarial training.\n",
        "    \"\"\"\n",
        "    # Loss function and optimizers.\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    loss_history = {\"G\": [], \"D\": []}\n",
        "\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss_G = 0.0\n",
        "        epoch_loss_D = 0.0\n",
        "\n",
        "        for real_sequences in tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            batch_size = real_sequences.size(0)\n",
        "            real_sequences = real_sequences.to(device)\n",
        "\n",
        "            # Convert real sequences (indices) to one-hot encoding.\n",
        "            real_onehot = F.one_hot(real_sequences, num_classes=vocab_size).float().to(device)\n",
        "\n",
        "            # Create labels.\n",
        "            real_labels = torch.ones(batch_size, 1, device=device)\n",
        "            fake_labels = torch.zeros(batch_size, 1, device=device)\n",
        "\n",
        "            # ---------------------------\n",
        "            # Train the Discriminator\n",
        "            # ---------------------------\n",
        "            optimizer_D.zero_grad()\n",
        "            output_real = discriminator(real_onehot)\n",
        "            loss_real = criterion(output_real, real_labels)\n",
        "\n",
        "            # Generate fake samples.\n",
        "            z = torch.randn(batch_size, latent_dim, device=device)\n",
        "            fake_onehot = generator(z)\n",
        "            output_fake = discriminator(fake_onehot.detach())\n",
        "            loss_fake = criterion(output_fake, fake_labels)\n",
        "\n",
        "            loss_D = loss_real + loss_fake\n",
        "            loss_D.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # ---------------------------\n",
        "            # Train the Generator\n",
        "            # ---------------------------\n",
        "            optimizer_G.zero_grad()\n",
        "            output_fake_for_G = discriminator(fake_onehot)\n",
        "            loss_G = criterion(output_fake_for_G, real_labels)\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            epoch_loss_D += loss_D.item()\n",
        "            epoch_loss_G += loss_G.item()\n",
        "\n",
        "        avg_loss_D = epoch_loss_D / len(data_loader)\n",
        "        avg_loss_G = epoch_loss_G / len(data_loader)\n",
        "        loss_history[\"D\"].append(avg_loss_D)\n",
        "        loss_history[\"G\"].append(avg_loss_G)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss_D: {avg_loss_D:.4f}, Loss_G: {avg_loss_G:.4f}\")\n",
        "\n",
        "    return loss_history\n"
      ],
      "metadata": {
        "id": "H4DsZNp018NV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Set the number of epochs and start training.\n",
        "num_epochs = 50\n",
        "print(\"Starting GAN training...\")\n",
        "loss_history = train_gan(generator, discriminator, data_loader, latent_dim, num_epochs, device, lr=0.0002)\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_ePw2wZ2Exk",
        "outputId": "9cac2895-2181-4264-8b76-f3881205488d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting GAN training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:39<00:00,  5.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 - Loss_D: 1.0325, Loss_G: 1.5342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/50 - Loss_D: 0.6443, Loss_G: 1.5820\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/50 - Loss_D: 0.6309, Loss_G: 1.7064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/50 - Loss_D: 0.4745, Loss_G: 1.9398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/50 - Loss_D: 0.4515, Loss_G: 2.2541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/50 - Loss_D: 0.5160, Loss_G: 2.2076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/50 - Loss_D: 0.7102, Loss_G: 2.0073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/50 - Loss_D: 0.7299, Loss_G: 1.9907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/50 - Loss_D: 0.3384, Loss_G: 2.1655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:40<00:00,  5.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50 - Loss_D: 0.1619, Loss_G: 2.7661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:40<00:00,  5.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/50 - Loss_D: 0.0813, Loss_G: 3.2380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:40<00:00,  5.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50 - Loss_D: 0.0411, Loss_G: 3.9228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:39<00:00,  5.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/50 - Loss_D: 0.0267, Loss_G: 4.5419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/50 - Loss_D: 0.0218, Loss_G: 4.8218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50 - Loss_D: 0.0202, Loss_G: 4.9788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/50 - Loss_D: 0.0195, Loss_G: 5.1006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/50 - Loss_D: 0.0188, Loss_G: 5.1823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/50 - Loss_D: 0.0178, Loss_G: 5.2963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:39<00:00,  5.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/50 - Loss_D: 0.0181, Loss_G: 5.3874\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:39<00:00,  5.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50 - Loss_D: 0.0178, Loss_G: 5.4995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:39<00:00,  5.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/50 - Loss_D: 0.0176, Loss_G: 5.5619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/50 - Loss_D: 0.0175, Loss_G: 5.5401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/50 - Loss_D: 0.0168, Loss_G: 5.6348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/50 - Loss_D: 0.0172, Loss_G: 5.6843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  5.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50 - Loss_D: 0.0170, Loss_G: 5.7287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/50 - Loss_D: 0.0169, Loss_G: 5.8401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:39<00:00,  5.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/50 - Loss_D: 0.0168, Loss_G: 5.8587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:39<00:00,  5.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/50 - Loss_D: 0.0167, Loss_G: 5.7811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:39<00:00,  5.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/50 - Loss_D: 0.0167, Loss_G: 5.9332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50 - Loss_D: 0.0166, Loss_G: 5.8295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  5.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50 - Loss_D: 0.0165, Loss_G: 5.9595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  5.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50 - Loss_D: 0.0165, Loss_G: 5.8829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  5.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/50 - Loss_D: 0.0165, Loss_G: 6.0247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  5.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/50 - Loss_D: 0.0164, Loss_G: 5.9159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/50 - Loss_D: 0.0163, Loss_G: 5.9078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/50 - Loss_D: 0.0163, Loss_G: 5.9377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  5.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/50 - Loss_D: 0.0163, Loss_G: 6.1184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/50 - Loss_D: 0.0162, Loss_G: 6.0104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/50 - Loss_D: 0.0162, Loss_G: 6.0350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/50 - Loss_D: 0.0162, Loss_G: 6.1130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  5.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/50 - Loss_D: 0.0161, Loss_G: 6.0860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  5.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/50 - Loss_D: 0.0163, Loss_G: 5.8749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  5.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/50 - Loss_D: 0.0165, Loss_G: 5.8964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  5.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/50 - Loss_D: 0.0174, Loss_G: 5.5868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  5.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/50 - Loss_D: 0.0172, Loss_G: 5.7540\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  5.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/50 - Loss_D: 0.0170, Loss_G: 5.8749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  5.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/50 - Loss_D: 0.0170, Loss_G: 6.1640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/50 - Loss_D: 0.0022, Loss_G: 6.1848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  5.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/50 - Loss_D: 0.0169, Loss_G: 6.2168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  5.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/50 - Loss_D: 0.0169, Loss_G: 6.1804\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Plot the training loss history.\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_history[\"D\"], label=\"Discriminator Loss\", marker='o')\n",
        "plt.plot(loss_history[\"G\"], label=\"Generator Loss\", marker='o')\n",
        "plt.title(\"Training Loss History\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "6YFlLFji2EhE",
        "outputId": "a4f1d652-17a5-4132-dc10-5fe896e6d3d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ed1219f434ab>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Plot the training loss history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"D\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Discriminator Loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"G\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Generator Loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# %% [markdown]\n",
        "# ## 5. Molecule Generation, Evaluation, and Visualization\n",
        "#\n",
        "# With our trained generator, we now:\n",
        "#\n",
        "# 1. **Generate Molecules:**  \n",
        "#    Use the Generator to produce new one-hot encoded SMILES sequences from random latent vectors.\n",
        "#\n",
        "# 2. **Decode SMILES:**  \n",
        "#    Convert one-hot encoded outputs back into SMILES strings.\n",
        "#\n",
        "# 3. **Validate Molecules:**  \n",
        "#    Use RDKit to check whether the generated SMILES are chemically valid.\n",
        "#\n",
        "# 4. **Visualize Molecules:**  \n",
        "#    Display a grid of valid molecules.\n",
        "#\n",
        "# Let's go through each step.\n"
      ],
      "metadata": {
        "id": "Xd1xRJ352Jnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "def onehot_to_smiles(onehot_seq: torch.Tensor, idx_to_char: Dict[int, str]) -> str:\n",
        "    \"\"\"\n",
        "    Converts a one-hot encoded sequence back to a SMILES string.\n",
        "    \"\"\"\n",
        "    # Get the token with the highest probability at each time step.\n",
        "    indices = torch.argmax(onehot_seq, dim=-1).cpu().numpy().tolist()\n",
        "    smiles = \"\".join([idx_to_char[i] for i in indices])\n",
        "    # Remove padding tokens.\n",
        "    smiles = smiles.replace(\"<PAD>\", \"\")\n",
        "    return smiles\n",
        "\n",
        "def generate_molecules(generator: nn.Module,\n",
        "                       latent_dim: int,\n",
        "                       num_samples: int,\n",
        "                       idx_to_char: Dict[int, str],\n",
        "                       device: torch.device) -> List[str]:\n",
        "    \"\"\"\n",
        "    Uses the Generator to produce new SMILES strings.\n",
        "    \"\"\"\n",
        "    generator.eval()\n",
        "    generated_smiles = []\n",
        "    with torch.no_grad():\n",
        "        # Generate latent noise.\n",
        "        z = torch.randn(num_samples, latent_dim, device=device)\n",
        "        fake_sequences = generator(z)\n",
        "        for seq in fake_sequences:\n",
        "            smiles = onehot_to_smiles(seq, idx_to_char)\n",
        "            generated_smiles.append(smiles)\n",
        "    return generated_smiles\n",
        "\n",
        "def validate_smiles(smiles_list: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates generated SMILES by attempting to parse them with RDKit.\n",
        "    \"\"\"\n",
        "    valid_smiles = []\n",
        "    for smi in smiles_list:\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is not None:\n",
        "            valid_smiles.append(smi)\n",
        "    print(f\"Validated {len(valid_smiles)} out of {len(smiles_list)} generated SMILES.\")\n",
        "    return valid_smiles\n",
        "\n",
        "def plot_molecules(smiles_list: List[str], mols_per_row: int = 5, img_size: tuple = (300, 300)) -> None:\n",
        "    \"\"\"\n",
        "    Plots a grid of valid molecules using RDKit and matplotlib.\n",
        "    \"\"\"\n",
        "    mols = []\n",
        "    for smi in smiles_list:\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol is not None:\n",
        "            mols.append(mol)\n",
        "    if not mols:\n",
        "        print(\"No valid molecules to plot.\")\n",
        "        return\n",
        "\n",
        "    grid_img = Draw.MolsToGridImage(mols, molsPerRow=mols_per_row, subImgSize=img_size)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(grid_img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Generated Molecules\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "mwzhvHfY2Eec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Generate new molecules using the trained generator.\n",
        "num_samples = 100\n",
        "generated_smiles = generate_molecules(generator, latent_dim, num_samples, idx_to_char, device)\n",
        "print(\"Some generated SMILES examples:\")\n",
        "for i, smi in enumerate(generated_smiles[:10]):\n",
        "    print(f\"{i+1}: {smi}\")\n"
      ],
      "metadata": {
        "id": "gSFnW88C2Eb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# Validate the generated SMILES.\n",
        "valid_generated_smiles = validate_smiles(generated_smiles)\n",
        "print(\"Some valid SMILES examples:\")\n",
        "for i, smi in enumerate(valid_generated_smiles[:10]):\n",
        "    print(f\"{i+1}: {smi}\")\n"
      ],
      "metadata": {
        "id": "_8CWBFFH2EZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0VbBm49cLix"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# %%\n",
        "# Plot the grid of valid generated molecules.\n",
        "plot_molecules(valid_generated_smiles, mols_per_row=5, img_size=(200, 200))\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Conclusion\n",
        "#\n",
        "# In this notebook we:\n",
        "#\n",
        "# - Downloaded and preprocessed a SMILES dataset.\n",
        "# - Built a custom PyTorch dataset for molecule generation.\n",
        "# - Defined and trained a GAN (Generator and Discriminator) with detailed monitoring.\n",
        "# - Generated, validated, and visualized new molecules.\n",
        "#\n",
        "# This detailed workflow demonstrates how GANs can be applied to the task of molecule generation using PyTorch. Feel free to adjust hyperparameters, network architectures, or evaluation metrics to explore further improvements.\n",
        "#\n",
        "# Happy molecule generation!\n"
      ]
    }
  ]
}